from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langgraph.graph import StateGraph, START
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from typing_extensions import TypedDict, List
import os

# Caminhos
persist_directory = "./chroma_db_openai"
collection_name = "docs"
pdf_dir = "./data"

# LLM e embeddings
llm = ChatOpenAI(model="gpt-4o", temperature=0.0, max_tokens=1000)
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# Vetoriza√ß√£o
if os.path.exists(persist_directory):
    print("üîÅ Carregando √≠ndice existente...")
    vector_store = Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=persist_directory,
    )
else:
    print("üìö Indexando PDFs...")
    loaders = [
        PyPDFLoader(os.path.join(pdf_dir, f))
        for f in os.listdir(pdf_dir)
        if f.endswith(".pdf")
    ]
    documents = []
    for loader in loaders:
        documents.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs_chunked = splitter.split_documents(documents)

    vector_store = Chroma.from_documents(
        documents=docs_chunked,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=persist_directory,
    )

# Prompt
prompt = ChatPromptTemplate.from_template("""
Voc√™ √© um assistente jur√≠dico especializado nas leis da CLT (Consolida√ß√£o das Leis do Trabalho), e responde de forma amig√°vel, clara e com base apenas nos documentos fornecidos.
<restricoes>
- N√£o use informa√ß√µes de fora do contexto.
- N√£o invente leis, artigos ou interpreta√ß√µes.
- N√£o responda perguntas fora do tema CLT (exceto sauda√ß√µes educadas).
- N√£o diga "Ol√°" e sauda√ß√µes educadas, exceto se algu√©m disser "oi" ou similar.
</restricoes>
Se algu√©m disser ‚Äúoi‚Äù ou fizer uma sauda√ß√£o, voc√™ pode responder com simpatia e lembrar que √© um assistente jur√≠dico sobre CLT.
Caso a resposta √† pergunta **n√£o esteja nos documentos fornecidos**, diga educadamente que **n√£o encontrou essa informa√ß√£o na legisla√ß√£o dispon√≠vel**.
Sempre que poss√≠vel, cite o nome do documento de onde a resposta veio.

<permissoes>
- Voc√™ pode responder perguntas sobre a CLT, direitos trabalhistas, contratos, f√©rias, rescis√£o, FGTS, jornada de trabalho, entre outros temas relacionadosa CLT
- Voc√™ pode explicar artigos, par√°grafos e incisos da CLT.
- Voc√™ pode fornecer informa√ß√µes sobre direitos e deveres de empregadores e empregados.
- Voc√™ pode esclarecer d√∫vidas sobre processos trabalhistas e como funcionam.
- Voc√™ pode ajudar a entender termos jur√≠dicos e procedimentos relacionados √† CLT.
- Voc√™ pode e deve lembrar que √© um assistente jur√≠dico especializado em CLT, e n√£o um advogado.
- Voc√™ pode e deve lembrar do nome e informa√ß√µes do usu√°rio, se fornecidas, para personalizar a conversa.
</permissoes>
---
- Hist√≥rico da conversa: {history}
- Pergunta: {question}
- Contexto: {context}
- Resposta:
""")

# State com hist√≥rico
class State(TypedDict):
    question: str
    context: List[Document]
    chat_history: List[dict]
    answer: str

# Retrieve
def retrieve(state: State):
    docs = vector_store.similarity_search(state["question"])
    return {"context": docs}

# Generate com hist√≥rico no prompt
def generate(state: State):
    # Formata hist√≥rico de chat
    history_lines = []
    for msg in state.get("chat_history", []):
        role = "Usu√°rio" if msg["role"] == "user" else "Assistente"
        history_lines.append(f"{role}: {msg['content']}")
    history_text = "\n".join(history_lines)

    # Formata contexto
    docs_text = "\n\n".join(doc.page_content for doc in state["context"])

    # Prompt
    messages = prompt.invoke({
        "question": state["question"],
        "context": docs_text,
        "history": history_text
    })
    response = llm.invoke(messages)
    return {"answer": response.content}

# Graph
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
